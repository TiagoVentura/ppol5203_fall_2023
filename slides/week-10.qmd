---
title: "<span style = 'font-size: 100%;'> PPOL 5203 - Data Science I: Foundations </span>"
subtitle: "<span style = 'font-size: 100%;'> Week 10: Introduction to Models and Statistical Learning </span>"
author: "Professor: Tiago Ventura"
date: "11/07/2023"
execute: 
  echo: false
  error: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1400
    height: 800
    center: false
    slide-number: true
    incremental: true
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Data science I: Foundations"
    theme: [simple, custom.scss]
---

# Questions about the homework? {background-color="#447099"}

## Plans for Today

-   Introduction to Machine Learning

    -   Statistical Learning

    -   Inference vs Predictive Modeling

    -   Machine Learning:

        -   Supervised vs Unsupervised
        -   Regression vs Classification
        -   Training models
        -   Bias and Variance Trade-off

    -   Coding

# Logistics

## Feedback survey

::: columns
::: {.column width="50%"}
### Continue Doing

::: nonincremental
-   small in-class practice with break

-   notebooks (you'll seem to love them!)
:::

### Stop doing

::: nonincremental
-   we're doing great!

-   super long problem sets
:::
:::

::: {.column width="50%"}
### Start doing

::: nonincremental
-   talk about past assignments

    -   Use office hours for this. Happy to discuss question if you have. But will not go over the entire assignment

-   readings could be posted at least 4 days

    -   my bad!

-   More clarity in problem set questions.

    -   Yes. Ask questions on slack if something is not clear.

-   Work in groups.

    -   please feel free to do it. Agree it works best.
:::
:::
:::

## Problem Set 4

-   Congress.gov is blocking scrappers

    -   Question 1 and 2 cannot be solved with `beautifulsoup`

        -   Solve with Selenium

        -   Solve Replacement Question

        -   Already have extra 10pts

-   Very flexible with deadline in the previous assignment. Start this problem set early, lot of moving pieces on this homework.

    -   No general extensions

    -   you will lose points for finishing late.

# Statistical Learning

## Statistical Models

Social Scientists are often interested in very complex questions. To answer them, we build models:

> A model is "a simplified representation of the reality created to serve a purpose." (Provost & Fawcett 2013)

By definition:

-   models are simplification

-   models highlight key aspects of our outcome of interest

-   models are all wrong, but some are useful

## Model's Components

The aim is to model the relationship between the outcome and some set of variables:

$$ y = f(X) + \sigma$$

**Where:**

-   $y$ is the outcome/dependent/response variable

-   $X$ is a matrix of predictors/features/independent variables

-   $f()$ is some fixed but unknown function mapping X to y. The signal in the data

-   $\sigma$ is some random error term. The "noise" in the data.

## Statistical Learning

Statistical learning refers to a set of methods/approaches for estimating $f(.)$

$$ \hat{y} = \hat{f}(X)$$

-   Where $\hat{f}(X)$ is an approximation of the "true" functional form, $f(X)$

-   $\hat{y}$ is the predicted value of a true value y.

-   When we build models, the aim is to find a $\hat{f}(X)$ that minimizes the error in the model.

## Reducible vs. Irreducible Error

$$E(y - \hat{y})^2$$ $$E[f(X) + \epsilon -  \hat{f}(X) ]^2$$

$$\underbrace{E[f(X) -\hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{var(\epsilon)}_{\text{Irreducible}}$$

-   The **"reducible" error** is the systematic **signal**. We can reduce this error by using different functional forms, better data, or a mixture of those two.

-   The **"irreducible" error** is associated with the random **noise** around $y$.

-   Statistical learning is concerned with minimizing the reducible error. However, our predictions will never be perfect given the irreducible error.

## Inference (Social Science) vs Prediction (Machine Learning)

Two reasons we want to estimate $f(\cdot)$:

-   **Inference**

    -   Goal is ***interpretation***

        -   *Which predictors are associated with the response?*
        -   *What is the relationship (parameters) between the response and the predictors?*
        -   *Is the relationship causal?*

    -   <font color = "darkred"> Key limitation</font>:

        -   using functional forms that are easy to interpret (e.g. lines) might be far away from the true function form of $f(X)$.

    -   **Classic Social Science Approach**

    -   $$ y_i = \beta_1*education_i + beta_2*gender_i + \sigma_i $$

## Inference (Social Science) vs Prediction (Machine Learning)

-   **Prediction**

    -   Goal is to ***predict*** values of the outcome, $\hat{y}$

    -   $\hat{f}(X)$ is treated as a ***black box***

        -   model doesn't need to be interpretable as long as it provides an accurate prediction of $y$.

    -   <font color = "darkred"> Key limitation</font>:

        -   *Interpretation*: it is difficult to know which variables are doing the heavy lifting and the exact influence of $x$ on $y$.

    -   **This is the Machine Learning tradition/predictive modeling/AI**

# Machine Learning

## Supervised and Unsupervised Learning

-   <u>**Supervised Learning (DS II)**</u>

    -   for each observation of the predictor measurement $x_i$ there is an associated response measurement $y_i$. In essence, there is an *outcome* we are aiming to accurately predict or understand.

    -   use regression and classification methods

::: fragment
```{r,echo=F,fig.align="center",fig.width=12}
require(tidyverse)
require(caret)
require(recipes)
set.seed(123)
x <- rnorm(150)
y = 1 + 2*x + rnorm(150)
tibble(x,y) %>% 
  ggplot(aes(x,y)) +
  geom_point(size=3,color="grey30") +
  geom_smooth(method="lm",se=F,size=2) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.title = element_text(size=18))
```
:::

## Unsupervised

-   <u>**Unsupervised Learning (DS III)**</u>

    -   we observe a vector of measurements $x_i$ but *no* associated response $y_i$.

    -   "unsupervised" because we lack a response variable that can supervise our analysis.

::: fragment
```{r,echo=F,fig.align="center",fig.width=12}
set.seed(123)
n = 50
sigma = matrix(c(1,0,0,1),ncol=2,nrow=2)
x1 <- MASS::mvrnorm(n,c(0,0),Sigma = sigma) %>% as_tibble() %>% mutate(group=1)
x2 <- MASS::mvrnorm(n,c(-3,-3),Sigma = sigma) %>% as_tibble() %>% mutate(group=2)
x3 <- MASS::mvrnorm(n,c(-3,3),Sigma = sigma) %>% as_tibble() %>% mutate(group=3)
bind_rows(x1,x2,x3) %>% 
  ggplot(aes(V1,V2,color=factor(group),pch=factor(group))) +
  geom_point(size=3) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.title = element_text(size=18))
```
:::

## Supervised Learning: Regression

*Outcomes* come in many forms. How the outcome is distributed will determine the methods we use.

-   **Quantitative** outcome

    -   a continuous/interval-based outcome: e.g. housing price, number of bills passed, stock market prices, etc.

    -   Regression Methods: linear, penalization, generalized additive models (GAMs)

    -   Both parametric and non-parametric ways of approximating $f(\cdot)$

------------------------------------------------------------------------

## Supervised Learning: Classification

*Outcomes* come in many forms. How the outcome is distributed will determine the methods we use.

-   **Qualitative** outcome

    -   a discrete outcome

        -   *Binary*: War/No War; Sick/Not Sick

        -   *Ordered*: Don't Support, Neutral, Support

        -   *Categorical*: Cat, Dog, Bus, ...

    -   Classification Methods: logistic regression, naive Bayes, support vector machines, neural networks

-   Some methods can be used on either outcome type

    -   K nearest neighbors
    -   tree-based methods (random forest, gradient boosting)

-   Every model has specific **tuning parameters** that we can use to optimize performance.

------------------------------------------------------------------------

## Interpretation vs Flexibility

::: columns
::: {.column width="50%"}
> "There is no free lunch in statistics"

-   No one method dominates all others over all possible data sets.

-   It is an important task to decide for any given set of data which method produces the best results

-   Balance between model interpretation and model flexibility
:::

::: {.column width="50%"}
```{r out.width="120%"}
knitr::include_graphics("week10_figs/interpret-vs-flexible.png")
```
:::
:::

## Building ML Model: What does learning mean?

For inference work (in you stats class), you get some data, and you estimate a model using the entire data. This is not what we do in machine learning.

-   **Main objective:** predict data accurately by learning from **previous seen**

-   **Goal:**

    -   capture signal in the best way possible $f(X)$

    -   Ignore noise (irreducible error)

## Train the model

The purpose of training the model is to **capture signal** and **ignore noise**.

To do so:

-   Define a accuracy metric:

    -   In the regression setting, the most common accuracy metric is *mean squared error* (MSE).
    -   $Mean Squared Error = \frac{\sum^N_{i=1} (y_i - \hat{f}(X_i))^2}{N}$

-   Train-Test Split

    -   Split your data between training and test

    -   Training data: find the best model and tune the parameters of these models

    -   Test data: assess the accuracy of your model.

-   Select the model based on smaller **out of sample predictive accuracy**. What is learned?

    -   Model type

    -   Features (variables)

    -   Hyper-Parameters

## Workflow

```{r}
knitr::include_graphics("https://cimentadaj.github.io/ml_socsci/img/socsci_wflow3_smaller.svg")
```

::: aside
source: https://cimentadaj.github.io/ml_socsci/
:::

## Challenge: avoid overfitting the data

```{r}
library(ggplot2)
library(patchwork)
library(scales)

## Data creation
set.seed(2313)
n <- 500
x <- rnorm(n)
y <- x^3 + rnorm(n, sd = 3)
age <- rescale(x, to = c(0, 100))
income <- rescale(y, to = c(0, 5000))
age_inc <- data.frame(age = age, income = income)
## Data creation

y_axis <- scale_y_continuous(labels = dollar_format(suffix = "â‚¬", prefix = ""),
                             limits = c(0, 5000),
                             name = "Income")

x_axis <- scale_x_continuous(name = "Age")

bad_fit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "lm") +
  y_axis +
  x_axis +  
  ggtitle("Underfit") +
  theme_minimal()

overfit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "loess", span = 0.015) +
  y_axis +
  x_axis +  
  ggtitle("Overfit") +
  theme_minimal() 

goodfit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "loess", span = 0.9, color="red") +
  y_axis +
  x_axis +  
  ggtitle("Ideal fit") +
  theme_minimal() +
  theme(plot.title = element_text(colour = "red", face = "bold"))


bad_fit + overfit + goodfit
```

## MSE Training Data

```{r}
library(tidyverse)
set.seed(123)
N = 100
x <- rnorm(N)
X <- splines::bs(x,degree=7)
B <- runif(7,-5,5) 
y <- X%*%B + rnorm(N,mean = 0,.2)
D = tibble(x,y = as.numeric(y))


modA = lm(y ~ x,data = D)
D$yhatA = predict(modA,D)
modD = lm(y ~ poly(x,21),data = D)
D$yhatD = predict(modD,D)
modB = lm(y ~ poly(x,2),data = D)
D$yhatB = predict(modB,D)
modC = lm(y ~ poly(x,5),data = D)
D$yhatC = predict(modC,D)
mse = function(y,yh) { mean((y-yh)^2) }

D %>% 
  gather(mod,yh,-y,-x) %>% 
  mutate(mod = str_remove(mod,"yhat")) %>% 
  group_by(mod) %>% 
  mutate(mse =mean((y-yh)^2),
         mse_txt = str_glue("Model {mod}\nMSE = {round(mse,3)}")) %>% 
  ggplot(aes(x,y)) +
  geom_point(alpha=.7,size=3,color="grey30") +
  geom_line(aes(x,yh),color="blue",size=2,alpha=.6) +
  theme_bw() +
  facet_wrap(~mse_txt) +
  theme(legend.position = "none",
        axis.title = element_text(size=18),
        axis.text = element_text(size=20),
        strip.text = element_text(size=24),
        text = element_text(family="serif"))


```

::: aside
**source: Dr. Eric Dunford's PPOL564 slides**
:::

## Model Accuracy: Out-Sample Prediction

```{r}
set.seed(321)
N = 100
x <- rnorm(N)
X <- splines::bs(x,degree=7)
# Use the same betas
y <- X%*%B + rnorm(N,mean = 0,.5)
D2 = tibble(x,y = as.numeric(y))

D2$yhatA = predict(modA,D2)
D2$yhatB = predict(modB,D2)
D2$yhatC = predict(modC,D2)
D2$yhatD = predict(modD,D2)

# Define Sets
D$set = "Training Set"
D2$set = "Test Set"

D2 %>% 
  gather(mod,yh,-y,-x,-set) %>% 
  mutate(mod = str_remove(mod,"yhat")) %>% 
  group_by(mod) %>% 
  mutate(mse =mean((y-yh)^2),
         mse_txt = str_glue("Model {mod}\nMSE = {round(mse,3)}")) %>% 
  ggplot(aes(x,y)) +
  geom_point(alpha=.7,size=3,color='forestgreen') +
  geom_line(aes(x,yh),color="blue",size=2,alpha=.6) +
  theme_bw() +
  facet_wrap(~mse_txt,scale="free") +
  # ylim(-5,5) +
  theme(legend.position = "none",
        axis.title = element_text(size=18),
        axis.text = element_text(size=20),
        strip.text = element_text(size=24),
        text = element_text(family="serif"))


```

::: aside
**source: Dr. Eric Dunford's PPOL564 slides**
:::

## Bias and Variance Trade-off

```{r out.width="100%"}
knitr::include_graphics("week10_figs/interpret-vs-flexible.png")
```

-   **high variance**: new data, new pattern.

-   **high bias**: rigid pattern, doesn't reflect the data

-   **Trade-off**: as we reduce bias, we risk overfiting. Trainning properly is key

## Cross-Validation

If you go back and forth between training and test to decide which model to use, you might also be overfiting. This is the idea of data leakage.

-   We can use re-sampling techniques to generate estimates for the test error.

-   "Re-sampling" involves repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.

-   We can use re-sampling techniques to generate estimates for the test error.

## K-Fold Cross Validation

-   K-Fold Cross Validation involves randomly dividing the data into $k$ groups (or folds). Model is trained on $k-1$ folds, then test on the remaining fold.

-   Process is repeated $k$ times, each time using a new fold. Offers $k$ estimates of the test error, which we average.

::: fragment
```{r out.width="100%"}
knitr::include_graphics("week10_figs/KfoldCV.gif")
```
:::

## Important Concepts

-   Inference vs Prediction

-   Prediction: black-box function, highly flexible to reduce error.

-   Learning: train the model in seen data. Test and calculate metrics in seen

-   Avoid overfiting and data leakage by using resampling methods.
