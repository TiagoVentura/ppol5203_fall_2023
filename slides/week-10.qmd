---
title: "<span style = 'font-size: 100%;'> PPOL 5203 - Data Science I: Foundations </span>"
subtitle: "<span style = 'font-size: 100%;'> Week 10: Introduction to Models and Statistical Learning </span>"
author: "Professor: Tiago Ventura"
date: "11/07/2023"
execute: 
  echo: false
  error: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1400
    height: 800
    center: false
    slide-number: true
    incremental: false
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Data science I: Foundations"
    theme: [simple, custom.scss]
---

## Plans for Today

-   Introduction to Machine Learning

    -   Statistical Models

    -   Inference vs Predictive Modelling

    -   Statistical Learning:

        -   Supervised vs Unsupervised
        -   Regression vs Classification
        -   Bias and Variance Trade-off
        -   Training models
        
    -  Application of ML in Social Sciences
    
    -  Coding


## Some questions we are interested as social scientist...

- [Which factors affect the likelihood of a country backslide to autocracy?](https://muse.jhu.edu/article/16730)

- [Why countries go to war?](https://www.math.fsu.edu/~mesterto/NewCourses/MAP5932/2016/PDF/PDF13/Fearon1995aExplanationsForWar.pdf)

- [Detect corruption or fraud in elections using official documents](https://www.cambridge.org/core/journals/political-analysis/article/fraudulent-democracy-an-analysis-of-argentinas-infamous-decade-using-supervised-machine-learning/EF5F7AC3A74E7F89494E478A05BFA8BC)

- [Predict/detect the sentiment/toxicity/presence of misinformation on a social media post](https://dl.acm.org/doi/abs/10.1007/978-3-031-26390-3_14)

- [Estimating socio-demographics of a region where census data is not available](https://www.unhcr.org/innovation/wp-content/uploads/2016/11/blumenstock-science-2015.pdf)

## Statistical Models

All these questions are complex. To answer them, we build models: 

> A model is “a simplified representation of reality created to serve a purpose.” (Provost & Fawcett 2013)

By definition: 

- models are simplification of a reality

- highlight key aspects of our outcome of interest

- are all wrong, but some are useful

## A Model and its components

The aim is to model the relationship between the outcome and some set of features features

$$ y = f(X) + \sigma$$

**Where:**

- $y$ is the outcome/dependent/response variable

- $X$ is a matrix of predictors/features/independent variables

- $f()$ is some fixed but unknown function mapping X to y. The signal in the data

- $\sigma$  is some random error term. The "noise" in the data.

## Statistical Learning

Statistical learning refers to a set of methods/approaches for estimating $f(.)$

$$ \hat{y} = \hat{f}(X) + \sigma$$


Where $\hat{f}(X)$ is an approximation of the "true" functional form, $f(X)$ and $\hat{y}$ is the predicted value of a true value y. 

When we build models, the aim is to find a $\hat{f}(X)$ that minimizes the reducible error in the model:

## Reducible vs. Irreducible Error

$$E(y - \hat{y})^2$$
$$E[f(X) + \epsilon -  \hat{f}(X)]^2$$

$$\underbrace{E[f(X) -\hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{var(\epsilon)}_{\text{Irreducible}}$$

The **"reducible" error** is the systematic **signal**. We can reduce this error by using different functional forms, better data, or a mixture of those two. 

The **"irreducible" error** is associated with the random **noise** around $y$. 

Statistical learning is concerned with minimizing the reducible error. However, our predictions  will never be perfect given the irreducible error. 

There is a lower bound on how accurate we can be.

## Inference (Social Science) vs Prediction (Machine Learning)

Two reasons we want to estimate $f(\cdot)$: 

- **Inference**

  + Goal is **_interpretation_**
  
      - _Which predictors are associated with the response?_
      - _What is the relationship (parameters) between the response and the predictors?_
      - _Is the relationship causal?_
      
  + **<font color = "darkred"> Key limitation</font>**: 
  
      - using functional forms that are easy to interpret (e.g. lines) might be far away from the true function form of $f(X)$.
      
  + **Classic Social Science Approach** 
  
$$ y_i = \beta_1*education_i + beta_2*gender_i + \sigma_i $$
  

## Inference (Social Science) vs Prediction (Machine Learning)

- **Prediction**

  + Goal is to **_predict_** values of the outcome, $\hat{y}$
  
  + $\hat{f}(X)$ is treated as a **_black box_**
      + model doesn't need to be interpretable as long as it provides an accurate prediction of $y$.
  
  + **<font color = "darkred"> Key limitation</font>**: 
  
      - *Interpretation*: it is difficult to know which variables are doing the heavy lifting and the exact influence of $x$ on $y$.
      
  + **This is the Machine Learning tradition, or, statistical learning**    
 

# Statistical Learning

## Supervised and Unsupervised Learning

- <u>**Supervised Learning**</u> 

  - for each observation of the predictor measurement $x_i$ there is an associated response measurement $y_i$. In essence, there is an _outcome_ we are aiming to accurately predict or understand.
  
  - use regression and classification methods
  
```{r,echo=F,fig.align="center",fig.width=7,fig.height=4}
require(tidyverse)
require(caret)
require(recipes)
set.seed(123)
x <- rnorm(150)
y = 1 + 2*x + rnorm(150)
tibble(x,y) %>% 
  ggplot(aes(x,y)) +
  geom_point(size=3,color="grey30") +
  geom_smooth(method="lm",se=F,size=2) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.title = element_text(size=18))
```

## Unsupervised

- <u>**Unsupervised Learning**</u>  

  - we observe a vector of measurements $x_i$ but _no_ associated response $y_i$.
  
  - "unsupervised" because we lack a response variable that can supervise our analysis.
  
```{r,echo=F,fig.align="center",fig.width=7,fig.height=4}
set.seed(123)
n = 50
sigma = matrix(c(1,0,0,1),ncol=2,nrow=2)
x1 <- MASS::mvrnorm(n,c(0,0),Sigma = sigma) %>% as_tibble() %>% mutate(group=1)
x2 <- MASS::mvrnorm(n,c(-3,-3),Sigma = sigma) %>% as_tibble() %>% mutate(group=2)
x3 <- MASS::mvrnorm(n,c(-3,3),Sigma = sigma) %>% as_tibble() %>% mutate(group=3)
bind_rows(x1,x2,x3) %>% 
  ggplot(aes(V1,V2,color=factor(group),pch=factor(group))) +
  geom_point(size=3) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.title = element_text(size=18))
```

## Interpretation vs Flexibility

:::: columns
::: {.column width="50%"}

> "There is no free lunch in statistics"

- No one method dominates all others over all possible data sets. 

- It is an important task to decide for any given set of data which method produces the best results

- Balance between  model interpretation and model flexibility
:::
::: {.column width="50%"}

```{r out.width="120%"}
knitr::include_graphics("week10_figs/interpret-vs-flexible.png")
```

:::
:::


## Building Models: Over-Fitting vs Under-Fitting

For inference work (in you stats class), you get some data, and you estimate a model using the entire data. This is not what we do in machine learning. 

#### What does learning mean? 

- **Main objective:** predict data accurately by learning from **previous seen**

- **Goal:** 

 - capture signal in the best way possible $f(X)$ 
 - Ignore noise (irreducible error) 


## Challenge: avoid overfitting the data

```{r}
library(ggplot2)
library(patchwork)
library(scales)

## Data creation
set.seed(2313)
n <- 500
x <- rnorm(n)
y <- x^3 + rnorm(n, sd = 3)
age <- rescale(x, to = c(0, 100))
income <- rescale(y, to = c(0, 5000))
age_inc <- data.frame(age = age, income = income)
## Data creation

y_axis <- scale_y_continuous(labels = dollar_format(suffix = "€", prefix = ""),
                             limits = c(0, 5000),
                             name = "Income")

x_axis <- scale_x_continuous(name = "Age")

bad_fit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "lm") +
  y_axis +
  x_axis +  
  ggtitle("Underfit") +
  theme_minimal()

overfit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "loess", span = 0.015) +
  y_axis +
  x_axis +  
  ggtitle("Overfit") +
  theme_minimal() 

goodfit <-
  ggplot(age_inc, aes(age, income)) +
  geom_point(alpha=.2) +
  geom_smooth(method = "loess", span = 0.9, color="red") +
  y_axis +
  x_axis +  
  ggtitle("Ideal fit") +
  theme_minimal() +
  theme(plot.title = element_text(colour = "red", face = "bold"))


bad_fit + overfit + goodfit
```


## Train the model

The purpose of training the model is to **capture signal** and **ignore noise**.

To do so: 

- Define a accuracy metric: 

  - In the regression setting, the most common accuracy metric is _mean squared error_ (MSE).
  - $MSE = \frac{\sum^N_{i=1} (y_i - \hat{f}(X_i))^2}{N}$

- Train-Test Split

  - Split your data between training and test
  
  - Training data: find the best model and tune the parameters of these models
  
  - Test data: assess the accuracy of your model. 
  
- Select the model based on smaller **out of sample predictive accuracy**

## Bias and Variance Trade-off

```{r out.width="100%"}
knitr::include_graphics("week10_figs/interpret-vs-flexible.png")
```


- **high variance**: new data, new pattern.

- **high bias**: rigid pattern, doesn't reflect the data 

- **Trade-off**: as we reduce bias, we risk overfiting. Trainning properly is key

## Cross-Validation

If you go back and forth between training and test to decide which model to use, you might also be overfiting. This is the idea of data leakage.

- We can use re-sampling techniques to generate estimates for the test error.

- "Re-sampling" involves repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.

- We can use re-sampling techniques to generate estimates for the test error.

## K-Fold Cross Validation

- K-Fold Cross Validation Involves randomly dividing the data into $k$ groups (or folds). Model is trained on $k-1$ folds, then test on the remaining fold. 

- Process is repeated $k$ times, each time using a new fold. Offers $k$ estimates of the test error, which we average. 


```{r out.width="100%"}
knitr::include_graphics("week10_figs/KfoldCV.gif")
```


## Important Concepts

- Inference vs Prediction

- Prediction: black-box function, highly flexible to reduce error. 

- Learning: train the model in seen data. Test and calculate metrics in seen

- Avoid overfitting and data leakage by using resampling methods. 







