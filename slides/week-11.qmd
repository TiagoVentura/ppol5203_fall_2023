---
title: "<span style = 'font-size: 100%;'> PPOL 5203 - Data Science I: Foundations </span>"
subtitle: "<span style = 'font-size: 100%;'> Week 11: Text-As-Data I: Description and Topics </span>"
author: "Professor: Tiago Ventura"
date: "11/14/2023"
execute: 
  echo: false
  error: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1600
    height: 800
    center: false
    slide-number: true
    incremental: true
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Data science I: Foundations"
    theme: [simple, custom.scss]
---

## Plans for Today + Next Week

**Introduction to Text Analysis/Natural Language Processing**

-   Motivation

-   Overal Workflow

-   Overview of Text Analysis Methods


    -   Pre-Process Text
    -   Descriptive Analysis and Comparisons
    -   Unsupervised Learning with Text: 
         - Topic Models
    -   Supervised Learning with Text (Next Week)
         - Dictionary Methods
         - Machine Learning Pre-Trained Models
         - Applications of LLMs (Large Languague models)
    
    

## A significant part of how we produce and store data comes in text


```{r}
knitr::include_graphics("http://media3.washingtonpost.com/wp-dyn/content/graphic/2011/02/11/GR2011021100614.jpg")
```


## Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.brookings.edu/wp-content/uploads/2018/11/RTS10VM4.jpg") 
```

## Social Media

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("week1_figs/redes.png") 
```

## Online Media: News, Comments, Blogs, etc...

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://miro.medium.com/v2/resize:fit:1400/1*t2u_0FHoQSpBK8i7j1NOBg.png") 
```

## Why to Analyze Text?

-   Humans are great at understanding the meaning of a sentence or a document in-depth

-   Computers are better at understanding patterns, classify and describe content across millions of documents

## Principles of Text Analysis:

-   Substantive knowledge is essential

-   Text analysis augments humans -- does not replace us

-   All text models are wrong -- but some are useful

-   The best method depends on the task

-   Alway validate your models!

## Workflow

-   **Acquire textual data:**

    -   Existing corpora; scraped data; digitized text

-   **Preprocess the data:**

    -   Bag-of-words
    -   Reduce noise, capture signal

## Workflow 

-   **Apply method appropriate to research goal:**

    -   Descriptive Analysis:
        -   Count Words, Readability; similarity;
    -   Classify documents into unknown categories
        -   Document clustering
        -   Topic models
    -   Classify documents into known categories
        -   Dictionary methods
        -   Supervised machine learning
        -   Transfer-Learning - use models trained in text for other purposes
    -   Scale documents on latent dimension:
        -   ideology of the text

## Already did it!

-   **Acquire textual data:**
    -   Existing corpora; scraped data; digitized text

## Today:

-   **Preprocess the data:**

    -   Bag-of-words
    -   Reduce noise, capture signal

-   **Apply methods appropriate to research goal:**

    -   Descriptive Analysis:
        -   Count Words, Readability; similarity;
    -   Classify documents into unknown categories
        -   Topic models

## Next Week:

-   Classify documents into known categories
    -   Dictionary methods
    -   Supervised machine learning
    -   Transfer-Learning - use models trained in text for other purposes

# Very quick introdution! Want learn more? Register in my class next Spring!

## Bag-of-Words Assumption

- Represent text as an unordered set of words in a document. 

- While order is ignored, frequency 
matters!

:::{.fragment}
**From Text to Numbers: Bag-of-Words & Document-Feature Matrix**

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/figures/dfm.png") 
```

:::

## Pre-Processing Steps

-   **Tokenize:** break out larger chuncks of text in words (unigram) or n-grams

-   **Lowercase:** convert all to lower case

-   **Remove Noise:** stopwords, numbers, punctuation, function words

-   **Stemming:** chops off the ends of words

-   **Lemmatization:** doing things properly with the use of a vocabulary and morphological analysis of words


## Sparse vs Dense Representation (Word Embeddings)

<br>

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://cdn.sanity.io/images/vr8gru94/production/96a71c0c08ba669c5a5a3af564cbffee81af9c6d-1920x1080.png") 
```

# Text-as-Data methods

## Similarity between documents


-   Every row in the Document-Feature Matrix (or in a Word Embedding Matrix) is a (**many other possible**) numerical representation of the document.

-   These vectors can be judged using metrics of similarity

:::{.fragment}

**Cosine Similarity**:

$$\text{Sim}(A, B) = \frac{{A \cdot B}}{{\|A\| \|B\|}}$$

- Where:

   -   The $\cdot$ here means a dot product: $\sum_j \mathbf{a_j} \cdot \mathbf{b_j}$
   -   The vector norm $\mathbf{||A||} = \sqrt{\sum \mathbf{{a_j}^2}}$

:::

## Application: Text Reuse in Congressional Bills

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("sim.png") 
```


## Topic Models: LDA Approach

**Intuition:** Capture words that are more likely to occur together across a set of documents. Assign these words a probability of being part of a cluster (topic). Assign documents a probability of being associated of these clusters.

## Topic Models: LDA Approach

**Statistical Model**: Latent Dirichlet Allocation + Generative Statistical Modeling

    -   **Documents:** random mixture over latent topics

    -   **Topics:** probability distribution over words

## Topic Models: LDA Approach

**Generative:**

-   For each document:
  -   Step 1: Randomly choose a distribution over topics.
  -   Step 2: For each word in the document
    -  a.  Randomly choose a topic from the distribution over topics in step #1.
    -  b.  Randomly choose a word from the corresponding topic distribution over the vocabulary.

-   **Inference:**
   - Moving from random to good topics.  
   - Algorithm changes the assignment of words to topics in every iteration, and the distribution of topic. This process maximizes co-occurrence of words

## Abstract the Math

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png") 
```

::: aside
*source:* https://www.cs.columbia.edu/\~blei/papers/Blei2012.pdf
:::

## Many different types of topic models

-   **Structural topic model:** allow (1) topic prevalence, (2) topic content to vary as a function of document-level covariates (e.g., how do topics vary over time or documents produced in 1990 talk about something differently than documents produced in 2020?); implemented in stm in R (Roberts, Stewart, Tingley, Benoit)

-   **Correlated topic model:** way to explore between-topic relationships (Blei and Lafferty, 2017); implemented in topicmodels in R; possibly somewhere in Python as well!

-   **Keyword-assisted topic model:** seed topic model with keywords to try to increase the face validity of topics to what you're trying to measure; implemented in keyATM in R (Eshima, Imai, Sasaki, 2019)

-   **BertTopic:** BERTopic is a topic modeling technique that leverages transformers and TF-IDF to create dense clusters of words.

# Notebooks
