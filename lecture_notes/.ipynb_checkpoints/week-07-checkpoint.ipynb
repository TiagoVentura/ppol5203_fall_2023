{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> PPOL 5203 Data Science I: Foundations <br><br> \n",
    "<font color='grey'> Parsing Unstructured Data: Scrapping Part I<br><br>\n",
    "Tiago Ventura</center> <h1> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In the class today, we will focus on:\n",
    "\n",
    "- Understand different strategies to acquire digital data\n",
    "- Understanding html structure to look up content on a website\n",
    "- Scrape content from a static website\n",
    "- Build a scraper to systematically draw content from similarly organized webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Digital information age\n",
    "\n",
    "We start our first lecture looking at this graph. It shows two things: \n",
    "\n",
    "- in the past few years we have produced and stored an enourmous among of data\n",
    "- Most of this data is produced and stored in digital environments. \n",
    "\n",
    "<div>\n",
    "<img src=\"http://media3.washingtonpost.com/wp-dyn/content/graphic/2011/02/11/GR2011021100614.jpg\" width=\"60%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all this data is available on digital spaces (like websites, social media apps, and digital archives). But some are. And as data scientists a primary skill that is expected from you is to be able to acquire, process, store and analyze this data. Today, we will focus on **acquiring data in the digital information era.** \n",
    "\n",
    "There are three primary techniques through which you can acquire digital data: \n",
    "\n",
    "- Scrap data from self-contained (static) websites\n",
    "- Scrap data from dynamic (javascript powered) websites\n",
    "- Access data through Application Programming Interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is scraping? \n",
    "\n",
    "**Scraping** consists of automatically collecting data available on websites. In theory, you can collect website data  by hand, or asking a couple of friends to help you. However, in a world of abundant data, this is likely not feasible, and in general, it may become more difficult once you have learned to collect it automatically.\n",
    "\n",
    "Let me give you some **examples of websites** I have alread scraped: \n",
    "\n",
    "- Electoral data from many different countries;\n",
    "- Composition of elites around the world;\n",
    "- Wikipedia; \n",
    "- Toutiao, a news aggregation from China; \n",
    "- Political Manifestos in Brazil \n",
    "- Fact-Checking News\n",
    "- Facebook and Youtube Live Chats. \n",
    "- Property Prices from Zillow. \n",
    "\n",
    "Scraping can be summarize in: \n",
    "\n",
    "- leveraging the structure of a website to **grab it's contents**\n",
    "\n",
    "- using a programming environment (such as R, Python, Java, etc.) to **systematically extract** that content.\n",
    "\n",
    "- accomplishing the above in an \"unobtrusive\" and **legal** way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping vs APIs\n",
    "\n",
    "\n",
    "An API is a set of rules and protocols that allows software applications to communicate with each other. APIs provide an front door for a developer to interact with a website. \n",
    "\n",
    "APIs are used for many different types of online communication and information sharing, among those, many **APIs have been developed to provide an easy and official way for developers and data scientists to access data**. \n",
    "\n",
    "As these APIs are developed by data owners, they are often secure, practical, and more organized than acquiring data through scrapping. \n",
    "\n",
    "Scraping is a back door for when there’s no API or when we need content beyond the structured fields the API returns\n",
    "\n",
    "**if you can use the API to access a dataset, that's where you will want to go**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Challenges with Scraping\n",
    "\n",
    "Webscraping is legal **as long as the scraped data is publicly available and the scraping activity does not harm the website being scraped**. These are two hugely relevant conditionals.  So before we begin coding, it is important to consider these issues.  \n",
    "\n",
    "Each call to a web server takes time, server cycles, and memory. Most servers can handle significant traffic, but they can't necessarily handle the strain induced by massive automated requests. Your code can overload the site, taking it offline, or causing the site administrator to ban your IP. \n",
    "\n",
    "We do not want to be seen as compromising the functioning of a website just because of our research. First, this overload can crash a server and prevent other users from accessing the site. Second, servers and hosters can, and do, implement countermeasures (i.e. block our access from our IP and so on). \n",
    "\n",
    "In addition, only collect public information. Think about Facebook. It is okay to collect public posts, or data from public groups. If by some way you manage to get into private groups, and group members have an expectation of privacy, it is not okay to collect their data. \n",
    "\n",
    "Here is a list of good practices for scraping:\n",
    "\n",
    "- Respect robots.txt\n",
    "- Don't hit servers too often\n",
    "- Slow down your code to the speed humans would manually do\n",
    "- Find trusted source sites\n",
    "- Do not shave during peak hours\n",
    "- Improve your code speed\n",
    "- Use data responsibly (As academics often do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Routine\n",
    "\n",
    "Scraping often involves the following routine: \n",
    "\n",
    "- **Step 1:** Find a website with information you want to collect\n",
    "- **Step 2:** Understand the website\n",
    "- **Step 3:** Write code to collect one realization of the data\n",
    "- **Step 4:** Build a scraper -- generalize you code into a function.\n",
    "- **Step 5:** Save\n",
    "\n",
    "And repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find a Website... but what is a website? \n",
    "\n",
    "A website in general is a combination of **HTML, CSS, XML, PHP, and Javascript**. We will care mostly about HTMLs and CSSs. \n",
    "\n",
    "\n",
    "### Static vs Dynamic Websites\n",
    "\n",
    "HTML forms what we call **static websites** - everything you see is there in the programming. Javascript produces dynamic sites - ones that you browse and click on and the url doesn't change - and are sites typically powered by a database deep within the programming. \n",
    "\n",
    "Today we will deal with static websites using the Python library ` Beautiful Soup`. For dynamic websites, we will learn next class about working with `selenium` in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Website\n",
    "\n",
    "HTML stands for **HyperText Markup Language**. As it is explict from the name, it is  a markup language used to create web pages and is a cornerstone technology of the internet. It is not a programming language as Python, R and Java.  Web browsers read HTML documents and render them into visible or audible web pages.\n",
    "\n",
    "See an example of an html file: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "<html>\n",
    "<head>\n",
    "  <title> Michael Cohen's Email </title>\n",
    "  <script>\n",
    "    var foot = bar;\n",
    "  <script>\n",
    "</head>\n",
    "<body>\n",
    "  <div id=\"payments\">\n",
    "  <h2>Second heading</h2>\n",
    "  <p class='slick'>information about <br/><i>payments</i></p>\n",
    "  <p>Just <a href=\"http://www.google.com\">google it!</a></p>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML code is structured using tags, and information is organized hierarchcially (like a list or an array) from top to bottom. \n",
    "\n",
    "Some of the most important tags we will use for scraping are: \n",
    "\n",
    "\n",
    "- **p** – paragraphs\n",
    "- **a href** – links\n",
    "- **div** – divisions\n",
    "- **h** – headings\n",
    "- **table** – tables\n",
    "\n",
    "See [here for more about html tags](https://betterprogramming.pub/understanding-html-basics-for-web-scraping-ae351ee0b3f9)\n",
    "\n",
    "### Scraping is all about finding tags and collecting the data associated with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Collect one realization of the data\n",
    "\n",
    "Let's \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
